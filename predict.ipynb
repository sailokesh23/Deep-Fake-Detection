{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8654da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sai lokesh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from mtcnn import MTCNN\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft, fftfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54755ae",
   "metadata": {},
   "source": [
    "# Extract Frames from Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7027d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved frame 0 from C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\id1_id0_0000.mp4\n",
      "Saved frame 2 from C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\id1_id0_0000.mp4\n",
      "Saved frame 4 from C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\id1_id0_0000.mp4\n",
      "Saved frame 6 from C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\id1_id0_0000.mp4\n",
      "Saved frame 8 from C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\id1_id0_0000.mp4\n",
      "Saved frame 10 from C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\id1_id0_0000.mp4\n",
      "Saved frame 12 from C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\id1_id0_0000.mp4\n",
      "Saved frame 14 from C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\id1_id0_0000.mp4\n",
      "Saved frame 16 from C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\id1_id0_0000.mp4\n",
      "Saved frame 18 from C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\id1_id0_0000.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_images_from_videos(videos_folder, output_folder, start_frame_number=601, num_images_per_video=10):\n",
    "    frame_number = start_frame_number\n",
    "    \n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for filename in os.listdir(videos_folder):\n",
    "        if filename.endswith(\".mp4\"):\n",
    "            video_path = os.path.join(videos_folder, filename)\n",
    "            frame_number = extract_images(video_path, output_folder, start_frame_number=frame_number, num_images=num_images_per_video)\n",
    "\n",
    "def extract_images(video_path, output_folder, num_images=30, start_frame_number=0):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = max(total_frames // num_images, 1)\n",
    "    count = 0\n",
    "    frame_number = start_frame_number\n",
    "\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return frame_number\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if count % interval == 0:\n",
    "            image_path = os.path.join(output_folder, f\"frame_{frame_number}.jpg\")\n",
    "            cv2.imwrite(image_path, frame)\n",
    "            print(f\"Saved frame {frame_number} from {video_path}\")\n",
    "            frame_number += 2  # Increment by two for every next image\n",
    "            if frame_number - start_frame_number >= num_images * 2:  \n",
    "                break\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    video.release()\n",
    "    return frame_number\n",
    "\n",
    "videos_folder = r\"C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\"\n",
    "output_folder = r\"C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\frame\"\n",
    "start_frame_number = 0\n",
    "num_images_per_video = 10\n",
    "\n",
    "extract_images_from_videos(videos_folder, output_folder, start_frame_number, num_images_per_video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab0543",
   "metadata": {},
   "source": [
    "# Extract faces and patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56f0ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face(image_path):\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(\"Error: Image not found\")\n",
    "        return None\n",
    "\n",
    "    # Initialize the MTCNN detector\n",
    "    detector = MTCNN()\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = detector.detect_faces(image)\n",
    "\n",
    "    if not faces:\n",
    "        print(\"No faces detected in the image\")\n",
    "        return None\n",
    "\n",
    "     #  face with the highest confidence\n",
    "    highest_confidence_face = max(faces, key=lambda x: x['confidence'])\n",
    "\n",
    "    # Extract the bounding box coordinates\n",
    "    x, y, w, h = highest_confidence_face['box']\n",
    "    \n",
    "    face_context = image[y:y+h, x:x+w]\n",
    "\n",
    "    region_top = max(0, y - 139)  \n",
    "    region_bottom = min(image.shape[0], y)  \n",
    "    region_left = max(0, x - 139)  \n",
    "    region_right = min(image.shape[1], x)  \n",
    "    \n",
    "    patch = image[region_top:region_bottom, region_left:region_right]\n",
    "    return face_context,patch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "156eedcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n"
     ]
    }
   ],
   "source": [
    "folder_path =r\"C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\frame\"\n",
    "\n",
    "faces_folder = os.path.join(folder_path, \"faces\")\n",
    "patches_folder = os.path.join(folder_path, \"patches\")\n",
    "os.makedirs(faces_folder, exist_ok=True)\n",
    "os.makedirs(patches_folder, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"): \n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        result = extract_face(image_path)\n",
    "        if result is not None:\n",
    "            face, patch = result\n",
    "            if face is not None and patch is not None:\n",
    "                face_filename = os.path.join(faces_folder, f\"face_{filename}\")\n",
    "                cv2.imwrite(face_filename, face)\n",
    "                \n",
    "                patch_filename = os.path.join(patches_folder, f\"patch_{filename}\")\n",
    "                cv2.imwrite(patch_filename, patch)\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                print(f\"No face or patch found in {filename}\")\n",
    "        else:\n",
    "            print(f\"Error processing {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f739e5",
   "metadata": {},
   "source": [
    "# Extraction of Noise features of faces and patches using inception v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3af41c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e13aafed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((299, 299))  # Resize to match Inception v3 input size\n",
    "    image = np.array(image)\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "# Function to extract features using Inception v3\n",
    "def extract_features(images):\n",
    "    \n",
    "    features = base_model.predict(images)\n",
    "    return features\n",
    "\n",
    "# Paths to your folders\n",
    "face_folders = [\n",
    "    (\"fake\", r\"C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\frame\\faces\"),\n",
    "    \n",
    "]\n",
    "\n",
    "patch_folders = [\n",
    "    (\"real\",r\"C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict\\frame\\patches\"),\n",
    "]\n",
    "\n",
    "# Lists to store extracted features\n",
    "face_features = []\n",
    "patch_features = []\n",
    "\n",
    "# Function to process folders\n",
    "def process_folders(folders, features_list):\n",
    "    for category, folder_path in folders:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
    "                image_path = os.path.join(folder_path, filename)\n",
    "                image = load_and_preprocess_image(image_path)\n",
    "                features = extract_features(np.expand_dims(image, axis=0))\n",
    "                features_list.append((category, features))\n",
    "\n",
    "# Process face folders\n",
    "process_folders(face_folders, face_features)\n",
    "\n",
    "# Process patch folders\n",
    "process_folders(patch_folders, patch_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86567c8b",
   "metadata": {},
   "source": [
    "# Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd0f781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(features, filename):\n",
    "    \n",
    "    categories, feature_arrays = zip(*features)\n",
    "    # Convert categories to numpy array\n",
    "    categories = np.array(categories)\n",
    "    # Convert feature arrays to numpy array\n",
    "    features_array = np.array(feature_arrays)\n",
    "    # Save features and categories\n",
    "    np.savez(filename, categories=categories, features=features_array)\n",
    "\n",
    "    \n",
    "    \n",
    "# Save face features\n",
    "save_features(face_features, r\"C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict_faces.npz\")\n",
    "\n",
    "# Save patch features\n",
    "save_features(patch_features, r\"C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict_patches.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0399e600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load face features\n",
    "face_data = np.load(r\"C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict_faces.npz\")\n",
    "face_categories = face_data['categories']\n",
    "face_feature= face_data['features']\n",
    "\n",
    "# Zip categories and features together\n",
    "face_features = list(zip(face_categories, face_feature))\n",
    "\n",
    "# Load patch features\n",
    "patch_data = np.load(r\"C:\\Users\\Sai lokesh\\OneDrive\\Documents\\jupyter\\predict_patches.npz\")\n",
    "patch_categories = patch_data['categories']\n",
    "patch_feature = patch_data['features']\n",
    "patch_features = list(zip(patch_categories, patch_feature))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8cde55",
   "metadata": {},
   "source": [
    "# Frequency Domain Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fb49b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_fft(features):\n",
    "    \n",
    "  \n",
    "  # Check if it's a single feature or a batch\n",
    "    if len(features.shape) == 1:\n",
    "        \n",
    "        fft_result = fft(features)\n",
    "    else:\n",
    "    # Apply FFT to each feature vector in the batch\n",
    "        \n",
    "        fft_result = np.apply_along_axis(fft, 1, features)\n",
    "    return fft_result\n",
    "\n",
    "\n",
    "# Process face features\n",
    "face_features_fft = perform_fft(face_feature)\n",
    "\n",
    "# Process patch features (not used in subsequent steps based on your explanation)\n",
    "patch_features_fft = perform_fft(patch_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2823b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frequency_features(fft_result):\n",
    "    \n",
    "\n",
    "    \n",
    "    magnitude_spectrum = np.abs(fft_result)\n",
    "\n",
    "    \n",
    "    frequencies = np.arange(len(magnitude_spectrum)) \n",
    "    mean_frequencies = frequencies * magnitude_spectrum / np.sum(magnitude_spectrum)\n",
    "    cumulative_energy = np.cumsum(magnitude_spectrum)  # Cumulative energy across frequencies\n",
    "\n",
    "    # Return features as arrays\n",
    "    return {\n",
    "        \"magnitude_spectrum\": magnitude_spectrum,\n",
    "        \"mean_frequencies\": mean_frequencies,  # Array of mean frequencies\n",
    "        \"cumulative_energy\": cumulative_energy,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a14e3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_frequency_features = {}\n",
    "for i, feature_fft in enumerate(face_features_fft):\n",
    "  # Extract features for each face feature\n",
    "  face_frequency_features[i] = extract_frequency_features(feature_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4291097",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_frequency_features = {}\n",
    "for i, feature_fft in enumerate(patch_features_fft):\n",
    "  # Extract features for each patch feature\n",
    "  patch_frequency_features[i] = extract_frequency_features(feature_fft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a5c04",
   "metadata": {},
   "source": [
    "# Concatenate Noise and frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cfa13b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined features shape: (10, 4, 2048)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate features for each sample\n",
    "combined_features = []\n",
    "for i in range(len(face_feature)):\n",
    "    \n",
    "    \n",
    "    combined_feature = np.concatenate((face_feature[i], patch_feature[i], face_frequency_features[i][\"magnitude_spectrum\"], patch_frequency_features[i][\"magnitude_spectrum\"]))\n",
    "    \n",
    "    combined_features.append(combined_feature)\n",
    "\n",
    "# Convert to numpy array for better handling\n",
    "combined_features = np.array(combined_features)\n",
    "\n",
    "print(\"Combined features shape:\", combined_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c506623",
   "metadata": {},
   "source": [
    "# Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07f26a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model(\"siam_net_model4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33fb77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = combined_features.reshape(combined_features.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e765808",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e4494f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 72ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = loaded_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d83e2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_predictions = (predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f4cc8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02fec0",
   "metadata": {},
   "source": [
    "# Output\n",
    "1 indicates the video is deepfake.\n",
    "\n",
    "0 indicates the video is real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9997dba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 1\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(binary_predictions, return_counts=True)\n",
    "counts_dict = dict(zip(unique, counts))\n",
    "\n",
    "# Determine majority value\n",
    "majority_value = max(counts_dict, key=counts_dict.get)\n",
    "\n",
    "print(\"predicted:\", majority_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
